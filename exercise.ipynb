{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Welcome to the Machine Learning exercise of the CMS Data Analysis School 2020 \n",
    "\n",
    "Before we start with the actual exercise, the software as well as the notebook itself need to be set up.\n",
    "\n",
    "To do this, please **execute the following five code cells**. Especially the first one might need a few minutes to complete, so grab a ‚òïÔ∏è or a beverage of your choice, and already start reading through the introduction.\n",
    "\n",
    "**And most importantly, have fun üéâ!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# setup some directories and install newest software on SWAN (TensorFlow 2.3 etc)\n",
    "# note: you can ignore warnings about version incompatibilities\n",
    "!$PWD/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# setup notebook extensions (cell splitting and presentation mode)\n",
    "!jupyter nbextension enable splitcell/splitcell\n",
    "!jupyter nbextension enable rise/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, reload the page for extensions to become active!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# update the sys.path to prefer custom software\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pyv = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "sys.path.insert(0, os.path.abspath(f\"software/lib/python{pyv}/site-packages\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To verify that the setup worked, you can checkout the version of the TensorFlow package.\n",
    "\n",
    "If it's `2.3.0`, everything worked fine. If not, please [contact the exercise facilitators](https://discord.com/channels/720916736983171133/745610366012227606) (see instructions on introduction slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# check the tf version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# load custom css to make plots centered and the presentation mode prettier\n",
    "from IPython.core.display import HTML\n",
    "css_file = \"assets/notebook_styles.css\"\n",
    "HTML(\"<style>{}</style>\".format(open(css_file, \"r\").read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Presentation mode\n",
    "\n",
    "Now that everything is up and running, you can proceed with the exercise below.\n",
    "\n",
    "If you want, you can also switch into **presentation** mode by clicking on the small button with the bar chart icon labelled \"Enter/Exit RISE Slideshow\" in the task bar above.\n",
    "\n",
    "**Note**: If the button does not show up, you might need to reload the page once to setup newly enabled notebook extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CMSDAS 2020 - Machine Learning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<img src=\"assets/boosted_top_decay.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Facilitators\n",
    "  - [Marcel Rieger](mailto:marcel.rieger@cern.ch?subject=CMSDAS%20ML%20Exercise) (CERN)\n",
    "  - [Thea √Öarrestad](mailto:thea.aarrestad@cern.ch?subject=CMSDAS%20ML%20Exercise) (CERN)\n",
    "  - [Oleg Filatov](mailto:oleg.filatov@cern.ch?subject=CMSDAS%20ML%20Exercise) (DESY)\n",
    "  - [Jan Kieseler](mailto:jan.kieseler@cern.ch?subject=CMSDAS%20ML%20Exercise) (CERN)\n",
    "  - [Shah Rukh Qasim](mailto:shah.rukh.qasim@cern.ch?subject=CMSDAS%20ML%20Exercise) (CERN)\n",
    "\n",
    "- Get in touch:\n",
    "  - [Discord channel](https://discord.com/channels/720916736983171133/745610366012227606)\n",
    "\n",
    "- Resources:\n",
    "  - [Twiki page](https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideCMSDataAnalysisSchoolCERN2020MLShortExercise)\n",
    "  - [Repository](https://github.com/riga/cmsdas2020_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Our case study will be that of discriminating between jets produced by a hadronically decaying top quark which hadronizes, to jets produced by a light flavour quark or a gluon.\n",
    "\n",
    "If the top quark has a very high transverse momentum, the decay products of the top (one b jet and two quark jets stemming from the decaying W boson), will be merged into one single large jet, which is referred to as a **top jet**. Potentially, this jet can exhibit three distinct, resolvable *sub jets*, whereas a light quark or gluon jet only appears as one single, large jet without any significant substructure.\n",
    "\n",
    "The different appearance of these jets can be used as a handle to discriminate between them.  Being able to correctly identify top jets, and tell them apart from the overwhelming background of other light-flavored jets, is extremely important for many reasons.\n",
    "\n",
    "Since the top quark is so heavy, being the only fermion we know of with a mass on the order of the weak scale, several extensions of the Standard Model which attempt to solve the hierarchy problem predict large couplings of new, hitherto unobserved particles to top quarks. Weeding top quark jets out of the ocean of other jets is therefore crucial for many **New Physics** searches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"assets/top_vs_qcd.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aim and scope of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As motivated above, the target of this exercise is to identify jets originating from top quark decays (**top jets**) over a background of highly energetic **light jets**, i.e. jets emerging from light *up*, *down*, or *strange* quarks as well as gluons. Events involving mostly QCD interactions are a typical production mechanism.\n",
    "\n",
    "You are given a dataset with information on > 1M jets, containing **kinematic observables** as well a **truth** label (a float value, either 0 or 1) that describes the origin of the jet. You can find more information on the input data in the cells below.\n",
    "\n",
    "**Your task is to create a neural (NN) network model that, given a jet, uses its kinematic observables as input features to predict its most probable origin!**\n",
    "\n",
    "In an **extension** of the exercise, you can learn how to build a NN that serves multiple purposes at the same time. We will try to let the model also predict the four-vector of the initial quark - the actual physics object we are interested in for reconstructing the underlying hard interaction.\n",
    "\n",
    "Also, if you feel competitive üí™, it will be possibility to compare results of your trained model with other participants. Don't hesitate to get in touch through the [discord channel](https://discord.com/channels/720916736983171133/745610366012227606).\n",
    "\n",
    "The **main aim** of this exercise is not to write a perfectly working model that is tailored for one specific use case, but rather to introduce you to some of the fundamental concepts that are used in machine learning applications today. Although we cannot meet, learn and discuss in person, try to collaborate and share ideas, thoughts and issues with others. Being able to *talk* about complex and abstract machine learning concepts is another important goal of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NN terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Before we dive in, let's quickly introduce a minimal set of NN terminology. Please note that a full introduction into machine learning would be too exhaustive at this point. For more insights, browse through the ML tutorial slides linked on the [Twiki page](https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCMSDataAnalysisSchoolCERN2020MLShortExercise) or checkout one of the many resources that are publically available. A well written introduction that even includes code examples and interactive visualizations is this free online book at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"assets/nn_graph.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "- NNs are structured into so-called **layers** that contain **units**.\n",
    "- We denote the **input layer** with $l = 0$, and the **output layer** with $l = L$.\n",
    "- When the number of input features we give to the network is $N$, then the input layer will have $N$ units.\n",
    "- The grey circles in the graph denote **bias units**. Their value is always one to allow for some numeric independence (see below).\n",
    "- These $N$ input values are **forward passed** to layer $l = 1$. We do not intend to directly feed values into it, nor do we manually extract its output values - hence we call it **hidden layer**.\n",
    "- The forward pass involves **weights** - these are free parameters, which are updated during training, and can be understood as *fit parameters*. NNs with O(100k) parameters are not uncommon!\n",
    "- The input to the *i-th* unit in layer $l = 1$, $z_i^l$, is simply the scalar product of the vector of input features and a vector of weights $W_{ij}^{l-1}$,\n",
    "\\begin{align}\n",
    "z_i^l &= \\sum_j W_{ij}^{l-1} \\cdot a_j^{l-1} + b_i^{l-1}\\\\\n",
    "\\Rightarrow z^l &= W^{l-1} \\cdot a^{l-1} + b^{l-1}\n",
    "\\end{align}\n",
    "where the bias unit is considered as an additional input with value 1, multiplied by the weight $b_j^{l-1}$. Each particular value of $W_{ij}^{l-1}$ and $b_j^{l-1}$ is represented by arrows in the diagram. The second line uses a **vectorized formulation** over all $1 \\leq j \\leq N$ inputs.\n",
    "- The output $a_i^l$ of this unit is the value of an [**activation** function](https://en.wikipedia.org/wiki/Activation_function) $\\sigma$, evaluated at its input $z_i^l$. This reads\n",
    "$$\n",
    "\\begin{align}\n",
    "a^l = \\sigma^l(z^l) = \\sigma_l(W^{l-1} \\cdot a^{l-1} + b^{l-1})\n",
    "\\end{align}\n",
    "$$\n",
    "in the vectorial form. The same function is applied to all units in the same layer.\n",
    "- Just like what we did with the input features, these output values can be propagated through the entire network up to the output layer $l = L$. Here, we choose a clever activation function to output the response $y$ that we like the network to learn (more on this later).\n",
    "- As a whole, we can see the network output $y$ as a function of $x$, given $W$ and $b$. Thus, we can write\n",
    "$$\n",
    "\\begin{align}\n",
    "y = y(x | \\underbrace{W,b}_{\\equiv\\ \\omega}) = \\underbrace{(a^L \\circ a^{L-1} \\circ \\dots \\circ a^1)_\\omega}_\\text{model} (x)\n",
    "\\end{align}\n",
    "$$\n",
    "and identify the **model** as the concatenation of all layers given the free parameters $\\omega$.\n",
    "- Besides the clear mathematical construction and rules to perform the forward pass, you might notice some *room for choices*, such as the number of hidden layers, the amount of units per layer, or the activation functions. These are called **hyper-parameters** and it is your task to understand your input data as well as the problem you want to solve, and adjust these parameters in a educated fashion to optimize the network performance.\n",
    "- It is this exploration of the huge **space** of hyper-parameters what makes working with NNs complex and exciting at the same time üé≠!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training, overtraining and data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training\n",
    "\n",
    "A comprehensive desription and explanantion of the *art* of NN training requires a collection of concepts and mathematical proofs that would by far reach beyond the scope of this short exercise. However, if you intend to apply machine learning techniques in the future, it is **highly recommended** to dive into this fascinating topic. Especially the formulation and proof of the **backpropagation** algorithm is of major importance as it paved the way of actual **deep** neural networks! A good place to start is the free online book at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overtraining\n",
    "\n",
    "During training, a NN receives *examples* in the form of vectors of input features and produces a prediction which can be compared to the *true* value one would expect. Based on the difference between prediction and expectation (in whatever way *difference* is defined), the network receives an either good or bad feedback that is used to update its trainable parameters, ideally leading to an improvement in the next iteration.\n",
    "\n",
    "However, this process can have some caveats! When the amount of available training examples is very limited, chances are that they might not describe underlying probability distributions with sufficient precision. As a result, the NN might start to develop a bias towards this particular set of examples. Then, after the training phase, when the NN is requested to evaluate examples that it never *saw* before, its ability to infer predictions might differ greatly from what you observed during training. The model fails to **generalize**, which is referred to as **overtraining**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Data splitting\n",
    "\n",
    "There are several techniques to monitor and prevent overtraining. One **mandatory** monitoring technique is data splitting. In its easiest form, the entirety of examples is split into three datasets:\n",
    "\n",
    "- The *training* dataset is used for the actual training procedure.\n",
    "- The *validation* dataset is **not** used **for** the training itself, but **during** the training to immediately monitor the NN's ability to generalize.\n",
    "- All actual measurements are performed on an independent *testing* dataset.\n",
    "\n",
    "This splitting is applied throughout this exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "The input data consists of 1 million jets, originating from either\n",
    "  - hadronically decaying top quarks (this is our **signal** ‚úîÔ∏é), or\n",
    "  - dijet QCD events (our **background** ‚úò),\n",
    " \n",
    "and clustered using the $k_{T}$ algorithm with $\\Delta R$ = 0.8.\n",
    "\n",
    "<br />\n",
    "\n",
    "Data was generated using Phythia & Delphes, configured\n",
    "  - to collide protons at 14 TeV center-of-mass energy,\n",
    "  - to generate jets with a $p_{T}$ range of [550, 650] GeV (before hadronization ‚ùóÔ∏è), and\n",
    "  - **without** mixing in pileup events for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"assets/top_vs_qcd.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input features\n",
    "\n",
    "Per jet, you are given the four-vectors of up to **200** of its *constituents* (i.e., the particles that form the jet by means of clustering).\n",
    "\n",
    "   - These up to 800 values define your **input features**.\n",
    "   - Note that not all jets have that many constituents‚ùóÔ∏è\n",
    "   - To spare you the trouble of working with uneven (so-called *jagged*) arrays, these \"missing\" constituents vectors are filled with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### Training targets\n",
    "\n",
    "Per jet, you are provided 2 different training targets:\n",
    "\n",
    "  1. A flag that marks the true origin of the jet \n",
    "    - `1` for jets from top quark decays\n",
    "    - `0` for light jets from QCD events\n",
    "  2. The true four-vector of the initial particle (only for top quarks)\n",
    " \n",
    "For now, we will focus on the **1.** training target to perform a *classification* to answer the question: **Top or not?**\n",
    "\n",
    "(Spoiler: the second target is used later on to include an energy regression ü§´)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hint\n",
    "\n",
    "From now in, you might want to work *outside the presentation mode*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diving into the data\n",
    "\n",
    "Let's check out the data! It is stored in NumPy arrays across several files, with 50k jets per file. This way, prototyping and test runs are way quicker. You are given\n",
    "\n",
    "- 20 training files (`\"train\"`)\n",
    "- 8 validation files (`\"valid\"`)\n",
    "- 8 testing files (`\"test\"`)\n",
    "\n",
    "A few tools to perform recurrent tasks such as data loading are available in the dedicated `dasml` package. Let's import all packages we need during this exercise and load two training files and inspect the contents.\n",
    "\n",
    "(you can neglect messages from matplotlib about creating the font cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load the dasml package and other software\n",
    "# (this might take a few seconds on SWAN)\n",
    "import dasml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "import livelossplot as llp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the content of two \"train\" files\n",
    "c_vectors, true_vectors, labels = dasml.data.load(\"train\", start_file=0, stop_file=2)\n",
    "c_vectors.shape, true_vectors.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All arrays have 100k (2 x 50k) *rows* (dimension 0).\n",
    "\n",
    "- Per jet, we have up to 200 constituents (`c_vectors`) with 4 variables ($E$, $p_x$, $p_y$, $p_z$) each, thus `(200, 4)`.\n",
    "- Consistently, `true_vectors` only has 4 values per jet.\n",
    "- The `labels`, however, are single values.\n",
    "\n",
    "Let's create a few plots to get some insights into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some flags to make four-vector element access more verbose\n",
    "E, PX, PY, PZ = range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a histogram helper\n",
    "def plot_hist(arr, names=None, xlabel=None, ylabel=\"Entries\", filename=None, legend_loc=\"upper center\", **kwargs):\n",
    "    kwargs.setdefault(\"bins\", 20)\n",
    "    kwargs.setdefault(\"alpha\", 0.7)\n",
    "   \n",
    "    # consider multiple arrays and names given as a tuple\n",
    "    arrs = arr if isinstance(arr, tuple) else (arr,)\n",
    "    names = names or (len(arrs) * [\"\"])\n",
    "\n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for arr, name in zip(arrs, names):\n",
    "        bin_edges = ax.hist(arr, label=name, **kwargs)[1]\n",
    "        kwargs[\"bins\"] = bin_edges\n",
    "    \n",
    "    # legend\n",
    "    if any(names):\n",
    "        legend = ax.legend(loc=legend_loc)\n",
    "        legend.get_frame().set_linewidth(0.0)\n",
    "    \n",
    "    # styles and custom adjustments\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    " \n",
    "    if filename:\n",
    "        fig.savefig(filename)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Truth distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# distribution of truth labels\n",
    "plot_hist(labels, xlabel=\"Label distribution\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# energy distribution of the true top quark particle\n",
    "# remember, this is only available for top jets (zero otherwise)\n",
    "is_top = labels == 1\n",
    "plot_hist(true_vectors[is_top, E], xlabel=\"True energy / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# px distribution of the true particle\n",
    "plot_hist(true_vectors[is_top, PX], xlabel=\"True $p_x$ / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mass distribution of the true particle\n",
    "mass = (true_vectors[:, E]**2 - np.sum(true_vectors[:, PX:]**2, axis=1))**0.5 \n",
    "plot_hist(mass[is_top], xlabel=\"True mass / GeV\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, this is a top!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Input feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# number of constituents per jet\n",
    "# remember, missing constituents are filled with zeros, so we take the energy value as a marker\n",
    "n_c = np.count_nonzero(c_vectors[:, :, E], axis=1)\n",
    "plot_hist(n_c, xlabel=\"N constituents per jet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# energy distribution of all constituents\n",
    "e_c = c_vectors[:, :, E].flatten()\n",
    "# store a mask to remove zeros\n",
    "non_zero = e_c != 0\n",
    "plot_hist(e_c[non_zero], log=True, xlabel=\"Constituents energy / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# px distribution of all constituents, zeros removed with the mask defined above\n",
    "px_c = c_vectors[:, :, PX].flatten()\n",
    "plot_hist(px_c[non_zero], log=True, xlabel=\"Constituents $p_x$ / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# pz distribution of all constituents\n",
    "pz_c = c_vectors[:, :, PZ].flatten()\n",
    "plot_hist(pz_c[non_zero], log=True, xlabel=\"Constituents $p_z$ / GeV\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "Altough you were promised *up to* 200 constituents per jet, only a few of them seem to have more than 100 constituents!\n",
    "\n",
    "Expect these *findings*, but don't interpret anything as bad intention üòâ The work packages of large-scale analysis are often shared and spread among multiple people, working groups and institutes. Staying on top of things is naturally a complex part, so communication and documentation is - as always - key!\n",
    "\n",
    "Ok, so now that we understood the data, it would not make sense to include all these zeros in a network training. We can safely pick only the first, say, **120 constituents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quickshot: a minimal training and evaluation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a full-blown training setup, let's first do a quickshot. This helps us to understand how a model is built, trained, and eventually evaluated. We can also already define a few plot methods to assess the performance.\n",
    "\n",
    "For this purpose, we use TensorFlow with the Keras high level API in its [functional version](https://keras.io/guides/functional_api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define a preprocessing function that (e.g.) takes the\n",
    "# constiuents and returns an other representation of them\n",
    "# in this case, we select only the first 120 constituents and\n",
    "# flatten the resulting array from (..., 120, 4) to (..., 480,)\n",
    "def preprocess_constituents(constituents):\n",
    "    return constituents[:, :120].reshape((-1, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, for the training we need to convert the label to a \"one-hot\" representation\n",
    "# 0. -> [1., 0.]\n",
    "# 1. -> [0., 1.]\n",
    "def labels_to_onehot(labels):\n",
    "    labels = labels.astype(np.int32)\n",
    "    onehot = np.zeros((labels.shape[0], labels.max() + 1), dtype=np.float32)\n",
    "    onehot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model generating function\n",
    "# - 2 hidden layers\n",
    "# - 128 units each\n",
    "# - tanh activation\n",
    "# - 2 output units with softmax activation\n",
    "# (applies exp() to outputs and normalizes sum of all outputs to 1)\n",
    "def create_model():\n",
    "    x = tf.keras.Input(shape=(480,))\n",
    "    a1 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(x)\n",
    "    a2 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(a1)\n",
    "    y = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a2)\n",
    "    return tf.keras.Model(inputs=x, outputs=y, name=\"toptagging_quickshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what happens when we call it with zeros\n",
    "# note: here we create zeros with in the shape (1, 480)\n",
    "# where the leading one marks the *batch size*,\n",
    "# i.e. the number of examples that are simultaneously fed\n",
    "# into the network to benefit from clever vectorization\n",
    "t = model.predict(np.zeros((1, 480)))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a TensorFlow `Tensor` object - an [*eager*](https://www.tensorflow.org/guide/eager) tensor to be precise. It *almost* feels and behaves like a NumPy array, except for a few differences that you will stumble upon very soon on your own ...\n",
    "To extract a true NumPy representation of its data, feel free to call `t.numpy()`. However, for writing performant, GPU compatible NNs, the use of the `Tensor` API is mandatory.\n",
    "\n",
    "The return value is `[0.5, 0.5]`. This means that, given a vector of input features consisting only of zeros, the network is unsure whether to assign it to the signal class (top jets) nor to the background class (light jets). This is totally reasonable as we haven't trained it yet. So let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load more training, and also validation data\n",
    "c_vectors_train, _, labels_train = dasml.data.load(\"train\", stop_file=6)\n",
    "c_vectors_valid, _, labels_valid = dasml.data.load(\"valid\", stop_file=3)\n",
    "\n",
    "# run the preprocessing\n",
    "c_vectors_train = preprocess_constituents(c_vectors_train)\n",
    "c_vectors_valid = preprocess_constituents(c_vectors_valid)\n",
    "\n",
    "# create one-hot labels\n",
    "labels_train = labels_to_onehot(labels_train)\n",
    "labels_valid = labels_to_onehot(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# this means that the internal computational graph structure is built,\n",
    "# the loss function (the function that provides the feedback by comparing\n",
    "# expected and predicted result, more on that later), and metrics are\n",
    "# registered that are shown during the training\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training for 5 epochs (running through all data 5 times)\n",
    "model.fit(\n",
    "    c_vectors_train,\n",
    "    labels_train,\n",
    "    batch_size=200,\n",
    "    epochs=5,\n",
    "    callbacks=[llp.PlotLossesKerasTF(outputs=[llp.outputs.MatplotlibPlot(cell_size=(4, 2))])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with an accuracy of about 70%, which is already quite good for such a small network (and lot's of important things we did not even consider yet ...)!\n",
    "\n",
    "Let's check if the model generalized by evaluating the validation data and manually computing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all training and validation data again for ruther study\n",
    "predictions_train = model.predict(c_vectors_train)\n",
    "predictions_valid = model.predict(c_vectors_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the accuracy\n",
    "def calculate_accuracy(labels, predictions):\n",
    "    # while the labels (NumPy array) are one-hot encoded,\n",
    "    # each prediction (TF tensor) consists of two numbers whose sum is 1,\n",
    "    # so we interpret the prediction to be the signal when the second value (index 1) is > 0.5\n",
    "    # hence, we can use argmax\n",
    "    predicteds_top = np.argmax(predictions, axis=-1) == 1\n",
    "    labels_top = labels[:, 1] == 1\n",
    "    return (predicteds_top == labels_top).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = calculate_accuracy(labels_train, predictions_train)\n",
    "acc_valid = calculate_accuracy(labels_valid, predictions_valid)\n",
    "\n",
    "print(f\"train accuracy: {acc_train:.4f}\")\n",
    "print(f\"valid accuracy: {acc_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fairly similar, so for now, we don't seem to experience overtraining.\n",
    "\n",
    "We proceed by taking a look at the output distributions of the validation dataset, separated into signal and background components. Since we are dealing with a binary classification, and the sum of the two output values is normalized to one, it is sufficient to inspect just one of the output nodes. Since our goal is to identify signal, we look at the second column with index 1 (note that the same is considered in the accuracy calculation above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    (predictions_valid[labels_valid == 0], predictions_valid[labels_valid == 1]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the classification accuracy, we can study the *receiver operating characteristic* curve or **ROC** curve. It shows the relation between the true positive (jets *correctly* identified as top jets) and false positive rates (light jets *mistaken* as a top jets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to draw a ROC curve\n",
    "def plot_roc(labels, predictions, names=None, xlim=(0.01, 1), ylim=(1, 1e2)):   \n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"True positive rate\")\n",
    "    ax.set_ylabel(\"1 / False positive rate\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    ax.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_xlim(left=xlim[0], right=xlim[1])\n",
    "    ax.set_ylim(bottom=ylim[0], top=ylim[1])\n",
    "    plots = []\n",
    "\n",
    "    # treat labels and predictions as tuples\n",
    "    labels = labels if isinstance(labels, tuple) else (labels,)\n",
    "    predictions = predictions if isinstance(predictions, tuple) else (predictions,)\n",
    "    names = names or (len(labels) * [\"\"])\n",
    "    for l, p, n in zip(labels, predictions, names):\n",
    "        # linearize\n",
    "        l = l[:, 1]\n",
    "        p = p[:, 1]\n",
    "\n",
    "        # create the ROC curve and get the AUC\n",
    "        fpr, tpr, _ = roc_curve(l, p)\n",
    "        auc = roc_auc_score(l, p)\n",
    "        \n",
    "        # apply lower x limit to prevent zero division warnings below\n",
    "        fpr = fpr[tpr > xlim[0]]\n",
    "        tpr = tpr[tpr > xlim[0]]\n",
    "\n",
    "        # plot\n",
    "        plot_name = (n and (n + \", \")) + \"AUC {:.3f}\".format(auc)\n",
    "        plots.extend(ax.plot(tpr, 1. / fpr, label=plot_name))\n",
    "\n",
    "    # legend\n",
    "    legend = ax.legend(plots, [p.get_label() for p in plots], loc=\"upper right\")\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the roc plot\n",
    "plot_roc(\n",
    "    (labels_train, labels_valid),\n",
    "    (predictions_train, predictions_valid),\n",
    "    names=(\"train\", \"valid\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves above are produced by scanning potential values to cut on the network output and examining the resulting signal classification (true positive) and background mis-classification (false positive) rates.\n",
    "\n",
    "Naturally, a well performing network has a high true positive rate while keeping the (reciprocal) false positive rate at a reasonably low (high) level. For the choice of the axes above, this would lead to a curve that is bent towards the upper right corner. But be aware that other representations of the ROC curve exist which might look somewhat different (e.g. \"1 - false positive rate\" on the y-axis). Their message is, however, identical.\n",
    "\n",
    "A commonly used proxy that compiles the values for all possible cuts into one metric is the area-under-curve - **AUC**. A value of 1 signalizes a perfectly working network that allows for a cut value leading to 100% signal efficiency and 0% background contamination. Opposed to that, a value of 0.5 means that the two output distributions of signal and background events are probably fully overlapping, lacking the opportunity to apply a cut that would favor signal examples. A value of 0 has the same logical meaning as 1, but the definition of what is signal and background is flipped. Therefore, the distance from 0.5 is what actually matters here.\n",
    "\n",
    "A value around 0.75 is already quite decent, but there's still potential. You can try to beat this value in the full training setup below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "- Now we know how to build a simple model using TensorFlow and Keras.\n",
    "- We learned how to one-hot encode labels.\n",
    "- We performed a quick training using the `fit()` method of Keras models.\n",
    "- To ensure model generalization, we evaluated validation data with our trainined model.\n",
    "- We calculated accuracies and visualized the output distributions.\n",
    "- We learned about ROC curves, AUC values and how to plot / compute them.\n",
    "\n",
    "With these tools at hand, we can jump into the next section and build a custom training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more insights into the actual neural network training process, we will use Keras only to compose the model. For preprocessing, the definition of losses, and the training loop, we will use bare TensorFlow operations and tools.\n",
    "\n",
    "Also, we reconsider some of the choices we made above and incorporate a few techniques that improve the network training.\n",
    "\n",
    "Here are a few TensorFlow resources that might help you in the process:\n",
    "\n",
    "- [Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "- [Loading NumPy data](https://www.tensorflow.org/tutorials/load_data/numpy)\n",
    "- [Keras layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "- [Eager execution](https://www.tensorflow.org/guide/eager)\n",
    "- [Gradient tape and differentiation](https://www.tensorflow.org/guide/autodiff)\n",
    "- [Graphs and introduction to `tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)\n",
    "- [Better performance with `tf.function`](https://www.tensorflow.org/guide/function)\n",
    "- [Training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
    "- [TensorFlow 2 tutorial held at CERN](https://indico.cern.ch/event/882992/contributions/3721506/attachments/1994721/3327402/TensorFlow_2_Workshop_CERN_2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eager execution and graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with NumPy, we can interactively work with TensorFlow tensors. Each operation is executed *eagerly* as soon as the interpreter reaches and evaluates that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "t = tf.range(0., 10.)\n",
    "print(t)\n",
    "t = t * 2\n",
    "print(t)\n",
    "t = t + 1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, one might not be interested in intermediate results so the outcome of `t = t * 2` in line 3 is perhaps not required. Also, imagine the operation `(t * 2) + 1` is executed on a GPU. The content of `t` - not that many bytes in this example, but tensors can easily reach a couple MBs - is transferred to the GPU, together with the instructions to multiply each value by 2 and then adding 1. The output of this computation is sent back to the CPU where (e.g.) the Python interpreter can print the numbers as done in line 6.\n",
    "\n",
    "There is obviously no need to send back the result of `t * 2`. However, this is exactly what would happen in the example above. While this is a nice and intuitive way to prototype a new model, we somehow need a way to tell TensorFlow to compute a set of instructions as a whole, and that we are only interested in the final result. This is where **graphs** enter the equation.\n",
    "\n",
    "A computational graph describes the symbolic instructions that should be performed on certain input tensors (orange) to produce the result of a complex computation. These instructions are represented by `tf.Operation` objects (green), while the data flowing between them is contained in `tf.Tensor`'s (purple). The graph of the computation above would look like this:\n",
    "\n",
    "[![](https://mermaid.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)](https://mermaid-js.github.io/mermaid-live-editor/#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)\n",
    "\n",
    "To declare a computational graph, we can write a function and decorate it with `tf.function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def my_func(t):\n",
    "    print(\"new graph created\", t.dtype, t.shape)\n",
    "    t = t * 2\n",
    "    print(t)\n",
    "    t = t + 1\n",
    "    print(t)\n",
    "    return t\n",
    "\n",
    "t = tf.range(0., 10.)\n",
    "my_func(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output is exactly the same, but the intermediate tensors no longer have values attached to them. The first time we called `my_func` in line 10, a concrete graph was created that expects an input tensor with type `float32` and shape `(10,)`. In fact, when we repeat this call with an input tensor of identical type and shape, `my_func` is not even called, but TensorFlow uses the previously created graph and executes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_func(tf.range(10., 20.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No line `new graph created ...` is printed, implying that `my_func` is indeed not called!\n",
    "\n",
    "However, if we use a tensor with a different type or shape, a new graph is created and stored internally. This powerful feature is called **signature tracing** and you can read more about it [here](https://www.tensorflow.org/guide/function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_func(tf.range(10., 21.))\n",
    "print(\"---\")\n",
    "my_func(tf.range(10, 20, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these concepts at hand, we can go ahead and start building our data pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data pipeline and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created a method `preprocess_constituents` to select the first 120 constituents per jet and to merge the last two dimensions so that we can feed a NN that expects an input feature vector. This was sufficient as the `model.fit` method knows how to apply batching of input jets and how to repeat the dataset to train for more than one epoch.\n",
    "\n",
    "Using plain TensorFlow, we use a `tf.data.Dataset` object for this purpose. Let's write a function that returns a dataset for our training.\n",
    "\n",
    "But now, we include an important aspect of deep learning, namely **feature scaling** (FS)! To introduce FS, we first need to understand the concept of **numerical domains** in the context of NN applications.\n",
    "\n",
    "Our input data - a selection of four-vectors with values given in GeV - clearly comes from the domain of physics. As we have seen in the plots above, their numerical values range from -500 to 500 for $p_x$ and $p_y$, and up to 2000 for $E$ and $p_z$ values. We can use the abstract term *application domain* to describe these ranges. However, the domain of numbers being passed back and forth through the network is entirely different and can even vary depending on the architecture you pick! A classical feed forward network, such as the one we created above, and a plentora of techniques that were developed throughout the last decade(s) prefer values to be in a range between, say, -1 and 1, and we can call it *network domain*. This is just an example and somewhat larger values are certainly fine as well. But still, you get the idea that numerical application and network domains are *entirely different*.\n",
    "\n",
    "Numerically, the output of our *classification* model is still in the network domain and we simply interpret it as a binary classification decision, so we are safe on this end. Things get a bit more tricky when we perform a *regression* task that should predict the value of a physics quantitiy. We deal with this topic in the extension of this exercise üëæ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(kind, shuffle=False, repeat=1, batch_size=100, n_constituents=120, seed=None, **kwargs):\n",
    "    # first, we load the data as before, passing all unresolved keyword arguments\n",
    "    c_vectors, true_vectors, labels = dasml.data.load(kind, **kwargs)\n",
    "    \n",
    "    # first, we measure the mean and standard deviation of the raw input vectors,\n",
    "    # of course not taking into account missing constituents\n",
    "    # we will need them for the feature scaling later on\n",
    "    non_zero = c_vectors[:, :, E].flatten() > 0\n",
    "    means = tf.constant([\n",
    "        np.mean(c_vectors[:, :, v].flatten()[non_zero])\n",
    "        for v in (E, PX, PY, PZ)\n",
    "    ])\n",
    "    variances = tf.constant([\n",
    "        np.var(c_vectors[:, :, v].flatten()[non_zero])\n",
    "        for v in (E, PX, PY, PZ)\n",
    "    ])\n",
    "    stddevs = tf.maximum(variances, 1e-6)**0.5\n",
    "    \n",
    "    # then we apply the cut on the first n_constituents per jet\n",
    "    # this is prettly basic and can happen outside the data pipeline\n",
    "    c_vectors = c_vectors[:, :n_constituents]\n",
    "    \n",
    "    # one-hot encode labels\n",
    "    labels = labels_to_onehot(labels)\n",
    "    \n",
    "    # create a tf dataset\n",
    "    data = (c_vectors, true_vectors, labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # in the following, we amend the dataset object using methods\n",
    "    # that return a new dataset object *without* copying the data\n",
    "    \n",
    "    # apply shuffeling\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10 * batch_size, reshuffle_each_iteration=True, seed=seed)\n",
    "    \n",
    "    # apply repetition, i.e. start iterating from the beginning when the dataset is exhausted\n",
    "    ds = ds.repeat(repeat)\n",
    "    \n",
    "    # apply batching\n",
    "    if batch_size < 1:\n",
    "        batch_size = c_vectors.shape[0]\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    # store the original data for later access\n",
    "    ds._orig_data = data\n",
    "    \n",
    "    return ds, means, stddevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training dataset\n",
    "dataset_train, means_train, stddevs_train = create_dataset(\n",
    "    \"train\", shuffle=True, repeat=-1, batch_size=200, stop_file=2)\n",
    "\n",
    "# also load all validation data but disable batching for easier handling\n",
    "dataset_valid, _, _ = create_dataset(\"valid\", batch_size=-1, stop_file=2)\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature scaling in a custom Keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature scaling procedure as a custom keras layer\n",
    "# that has, of course, no weights as it is not trainable\n",
    "# see https://keras.io/guides/making_new_layers_and_models_via_subclassing for more info\n",
    "\n",
    "class FeatureScaling(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, means, stddevs):\n",
    "        \"\"\"\n",
    "        Constructor. Stores arguments as instance members.\n",
    "        \"\"\"\n",
    "        super(FeatureScaling, self).__init__(trainable=False)\n",
    "\n",
    "        self.means = means\n",
    "        self.stddevs = stddevs\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Method that is required for model cloning and saving. It should return a\n",
    "        mapping of instance member names to the actual members.\n",
    "        \"\"\"\n",
    "        return {\"means\": self.means, \"stddevs\": self.stddevs}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "        Method that, given an input shape, defines the shape of the output tensor.\n",
    "        This way, the entire model can be built without actually calling it.\n",
    "        \"\"\"\n",
    "        return (input_shape[0], input_shape[1] * input_shape[2])\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Any variables defined by this layer should be created inside this method.\n",
    "        This helps Keras to defer variable registration to the point where it is\n",
    "        needed the first time, and in particular not at definition time.\n",
    "        \"\"\"\n",
    "        # nothing to do here as our feature scaling has not trainable parameters\n",
    "\n",
    "    def call(self, c_vectors):\n",
    "        \"\"\"\n",
    "        Payload of the layer that takes inputs and computes the requested output\n",
    "        whose shape should match what is defined in compute_output_shape.\n",
    "        \"\"\"\n",
    "        # scale each feature such that it is distributed around 0 with a standard deviation of 1\n",
    "        # BUT: there are already many zeros in the input features and they have\n",
    "        #      a distinct meaning (missing constituents); we want to keep this information, so we\n",
    "        #      shift these values to -3, i.e. 3 standard deviations to the left\n",
    "        e, px, py, pz = tf.unstack(c_vectors, axis=-1)\n",
    "        zero_pos = -3. * tf.ones_like(e)\n",
    "        non_zero = e > 0\n",
    "        e = tf.where(non_zero, (e - self.means[E]) / self.stddevs[E], zero_pos)\n",
    "        px = tf.where(non_zero, (px - self.means[PX]) / self.stddevs[PX], zero_pos)\n",
    "        py = tf.where(non_zero, (py - self.means[PY]) / self.stddevs[PY], zero_pos)\n",
    "        pz = tf.where(non_zero, (pz - self.means[PZ]) / self.stddevs[PZ], zero_pos)\n",
    "\n",
    "        # we anyway need to flatten the vectors, so just concatenate components\n",
    "        features = tf.concat((e, px, py, pz), axis=-1)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, units=(128, 128, 128), activation=\"tanh\", dropout_rate=0., fs_args=None):\n",
    "    # track weights for later use\n",
    "    weights = []\n",
    "    \n",
    "    # input layer\n",
    "    x = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # feature scaling\n",
    "    if not fs_args:\n",
    "        fs_args = (tf.constant(4 * [0.]), tf.constant(4 * [1.]))\n",
    "    a = FeatureScaling(*fs_args)(x)\n",
    "\n",
    "    # add layers programatically\n",
    "    for n in units:\n",
    "        # build the layer\n",
    "        layer = tf.keras.layers.Dense(n, use_bias=True, activation=activation)\n",
    "        a = layer(a)\n",
    "\n",
    "        # store the weight matrix for later use\n",
    "        weights.append(layer.kernel)\n",
    "\n",
    "        # add random unit dropout\n",
    "        if dropout_rate:\n",
    "            a = tf.keras.layers.Dropout(dropout_rate)(a)\n",
    "\n",
    "    # add the softmax layer\n",
    "    y = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a)\n",
    "    \n",
    "    # build the model\n",
    "    model = tf.keras.Model(inputs=x, outputs=y, name=\"toptagging_custom\")\n",
    "\n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model, regularization_weights = create_model((120, 4), fs_args=(means_train, stddevs_train))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy\n",
    "\n",
    "The last ingredient before running the training loop is the definition of the loss function. Since we only use Keras for the model building process above, we are free to use anything we want!\n",
    "\n",
    "The main component of the loss is - as above - the binary cross entropy (CE) loss, which is a common choice for classification problems that use a softmax activation in the last layer. Although many variations of CE exist (e.g. the group of *focal* losses), we stick with this simple yet powerful formula,\n",
    "$$\n",
    "\\begin{align}\n",
    "L_\\text{CE}(y, y_t) = -y_t \\cdot \\log(y)\n",
    "\\end{align}\n",
    "$$\n",
    "where $y$ is the NN prediction and $y_t$ is the ground truth.\n",
    "\n",
    "We could have also used the Keras implementation which, in combination with an *unactivated* output layer, takes a shortcut around applying exponential functions in the output layer and building logarithms again in the loss. However, we are here to learn so we do this on our own üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "#### L2 regularization\n",
    "\n",
    "The second term in our loss function does not compare predicted and expected values, but only considers the values of all weights used in the model and provides a bad feedback in case these variables obtain rather high values. To understand why high variable values are discouraged in typical NN applications, you can imagine a simple fit of a 1-D function to a set of examples (see image below).\n",
    "\n",
    "<center><img src=\"assets/nn_capacity.png\" width=\"60%\"/></center>\n",
    "\n",
    "In case a network has too few parameters (case 1), its capacity is insufficient to describe the examples with good accuracy (*underfitting*). A network with appropriate capacity (case 2) describes the data in all parts of the phase space $x$. However, guessing the correct capacity that applies equally to all parts of the usually high-dimensional phase space is not always possible.\n",
    "\n",
    "Therefore, the scenario of *overfitting* becomes relevant (case 3). In some parts of the phase space (here, for low x values), the prediction of the network fluctuates significantly to explain each example. This is realized through large values of the paramters of the underlying fit model. The same observation holds for higher-dimensional fits and hence, also for neural networks.\n",
    "\n",
    "For this reason, we introduce the $L_2$ regularization loss, which simply sums up the squares (thus $_2$) of all traininable parameters. Before adding this term to the CE loss defined above, we scale it by a factor $\\lambda$ (`l2_norm`) to control the overall strength of the $L_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the losses\n",
    "def create_losses(weights, l2_norm=0.001):\n",
    "    # cross entropy\n",
    "    @tf.function\n",
    "    def loss_ce_fn(labels, predictions):\n",
    "        # ensure proper prediction values before applying log's\n",
    "        predictions = tf.clip_by_value(predictions, 1e-6, 1 - 1e-6)\n",
    "        loss_ce = tf.reduce_mean(-labels * tf.math.log(predictions))\n",
    "        return loss_ce\n",
    "\n",
    "    # l2 loss\n",
    "    @tf.function\n",
    "    def loss_l2_fn(labels, predictions):\n",
    "        # accept labels and predictions although we don't need them\n",
    "        # but this makes it easier to call all loss functions the same way\n",
    "        loss_l2 = sum(tf.reduce_sum(w**2) for w in weights)\n",
    "        \n",
    "        return l2_norm * loss_l2\n",
    "        \n",
    "    # return a dict with all loss function components\n",
    "    return {\"ce\": loss_ce_fn, \"l2\": loss_l2_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = create_losses(regularization_weights, l2_norm=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need an optimizer object that handles the propagation of derivatives back through the network and updates all trainable weights. There are many different optimizers out there, but for now, we will stick with the [*Adam*](https://arxiv.org/abs/1412.6980) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer with a variable learning rate\n",
    "def create_optimizer(initial_learning_rate=0.005):\n",
    "    learning_rate = tf.Variable(initial_learning_rate, dtype=tf.float32, trainable=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    return optimizer, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, learning_rate = create_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define the training loop. Here, we use the TensorFlow `GradientTape` which tracks all executed operations and provides the partial gradients of the loss function with respect to all traininable weights, that are used to update their values as part of the backpropagation algorithm. You can learn more on the `GradientTape` and custom training loops [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataset_train, dataset_valid, model, loss_fns, optimizer, learning_rate,\n",
    "                  max_steps=10000, log_every=10, validate_every=100, stack_energy=False): \n",
    "    # store the best model, identified by the best validation accuracy\n",
    "    best_model = None\n",
    "\n",
    "    # metrics to update during training\n",
    "    metrics = dict(\n",
    "        step=0, step_val=0,\n",
    "        acc_train=0., acc_valid=0., acc_valid_best=0.,\n",
    "        auc_train=0., auc_valid=0., auc_valid_best=0.,\n",
    "    )\n",
    "    for name in loss_fns:\n",
    "        for kind in [\"train\", \"valid\"]:\n",
    "            metrics[f\"loss_{name}_{kind}\"] = 0.\n",
    "    \n",
    "    # progress bar format\n",
    "    fmt = [\"{percentage:3.0f}% {bar} Step: {pfx[0][step]}/{total}, Validations: {pfx[0][step_val]}\"]\n",
    "    for name in loss_fns:\n",
    "        fmt.append(f\"Loss '{name}': {{pfx[0][loss_{name}_train]:.4f}} | {{pfx[0][loss_{name}_valid]:.4f}}\")\n",
    "    fmt.append(\"Accuracy: {pfx[0][acc_train]:.4f} | {pfx[0][acc_valid]:.4f} | {pfx[0][acc_valid_best]:.4f}\")\n",
    "    fmt.append(\"ROC AUC: {pfx[0][auc_train]:.4f} | {pfx[0][auc_valid]:.4f} | {pfx[0][auc_valid_best]:.4f}\")\n",
    "    fmt.append(\"(loss format: 'last train | last valid', metric format: 'last train | last valid | best valid')\")\n",
    "    fmt = \" --- \".join(fmt).replace(\"pfx\", \"postfix\")\n",
    "\n",
    "    # helper to update metrics\n",
    "    def update_metrics(bar, kind, step, labels, predictions, losses):\n",
    "        # calculate accuracy and roc auc\n",
    "        acc = calculate_accuracy(labels.numpy(), predictions.numpy())\n",
    "        auc = roc_auc_score(labels[:, 1], predictions[:, 1])\n",
    "        # update bar data\n",
    "        metrics[\"step\"] = step + 1\n",
    "        metrics[f\"acc_{kind}\"] = acc\n",
    "        metrics[f\"auc_{kind}\"] = auc\n",
    "        for name, loss in losses.items():\n",
    "            metrics[f\"loss_{name}_{kind}\"] = loss\n",
    "        # validation specific\n",
    "        if kind == \"valid\":\n",
    "            metrics[\"step_val\"] += 1\n",
    "            metrics[\"acc_valid_best\"] = max(metrics[\"acc_valid_best\"], acc)\n",
    "            metrics[\"auc_valid_best\"] = max(metrics[\"auc_valid_best\"], auc)\n",
    "            # return True when this was the best validation step\n",
    "            return acc == metrics[\"acc_valid_best\"]\n",
    "    \n",
    "    # start the loop\n",
    "    with tqdm(total=max_steps, bar_format=fmt, postfix=[metrics]) as bar:\n",
    "        for step, (c_vectors, true_vectors, labels) in enumerate(dataset_train):\n",
    "            if step >= max_steps:\n",
    "                print(f\"{max_steps} steps reached, stopping training\")\n",
    "                break\n",
    "                \n",
    "            # when stack_energy is set (see end of the exercise)\n",
    "            # stack the true energy on top of the labels\n",
    "            if stack_energy:\n",
    "                labels = tf.concat([labels, true_vectors[:, :E + 1]], axis=-1)\n",
    "\n",
    "            # do a train step\n",
    "            with tf.GradientTape() as tape:\n",
    "                # get predictions\n",
    "                predictions = model(c_vectors, training=True)\n",
    "                # compute all losses and combine them into the total loss\n",
    "                losses = {\n",
    "                    name: loss_fn(labels, predictions)\n",
    "                    for name, loss_fn in loss_fns.items()\n",
    "                }\n",
    "                loss = tf.add_n(list(losses.values()))\n",
    "            # get and propagate gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # logging\n",
    "            do_log = step % log_every == 0\n",
    "            if do_log:\n",
    "                update_metrics(bar, \"train\", step, labels, predictions, losses)\n",
    "\n",
    "            # validation\n",
    "            do_validate = step % validate_every == 0\n",
    "            if do_validate:\n",
    "                c_vectors_valid, true_vectors_valid, labels_valid = next(iter(dataset_valid))\n",
    "                if stack_energy:\n",
    "                    labels_valid = tf.concat([labels_valid, true_vectors_valid[:, :E + 1]], axis=-1)\n",
    "                predictions_valid = model(c_vectors_valid, training=False)\n",
    "                losses_valid = {\n",
    "                    name: loss_fn(labels_valid, predictions_valid)\n",
    "                    for name, loss_fn in loss_fns.items()\n",
    "                }\n",
    "                is_best = update_metrics(bar, \"valid\", step, labels_valid, predictions_valid, losses_valid)\n",
    "                \n",
    "                # store the best model\n",
    "                if is_best:\n",
    "                    best_model = tf.keras.models.clone_model(model)\n",
    "            \n",
    "            bar.update()\n",
    "\n",
    "        else:\n",
    "            log(\"dataset exhausted, stopping training\")\n",
    "\n",
    "    print(\"validation metrics of the best model:\")\n",
    "    print(f\"Accuracy: {metrics['acc_valid_best']:.4f}\")\n",
    "    print(f\"ROC AUC : {metrics['auc_valid_best']:.4f}\")\n",
    "    \n",
    "    return best_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model, metrics = training_loop(\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    model,\n",
    "    loss_fns,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    max_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Well, that doesn't look too good. Why?\n",
    "\n",
    "There are actually several reasons ...\n",
    "\n",
    "- We only loaded a fraction of the input data.\n",
    "- The training only ran for 2000 steps, i.e., 2000 forward pass and back propagation calls. Given the amount of data, this is clearly not enough.\n",
    "- None of the hyper-parameters is tuned yet.\n",
    "\n",
    "Now, it's up to you to improve the training! Perhaps also try to include further concepts. Good starting points are\n",
    "\n",
    "- [Learning rate scheduling](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay)\n",
    "- [Batch normalization](https://keras.io/api/layers/normalization_layers/batch_normalization)\n",
    "- [Activations](https://keras.io/api/layers/activations/#selu-function)\n",
    "- [Focal loss](https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7)\n",
    "- [...](https://lmgtfy.com/?q=How+to+improve+my+neural+network)\n",
    "\n",
    "and of course our [discord channel](https://discord.com/channels/720916736983171133/745610366012227606).\n",
    "\n",
    "Can you reach an accuracy of 85%? You can use the cells below which wrap all of the above settings and methods in less space.\n",
    "\n",
    "(If you reach 95% or more - without using truth information as input - contact us üòé)\n",
    "\n",
    "**Note**: If you experience notebook kernal interruptions or messages like `Allocation of XXXXXXXX exceeds XX% of free system memory` on the terminal, reduce the number of input files again with the `stop_file` parameter as we did above. Reasonble results can already be achieved with a subset of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "# ACTION REQUIRED\n",
    "n_constituents = 120\n",
    "batch_size = ....\n",
    "l2_norm = ....\n",
    "initial_learning_rate = ....\n",
    "units = ....\n",
    "activation = ....\n",
    "dropout_rate = ....\n",
    "n_train_files = -1  # set this to a value that works with your RAM\n",
    "n_valid_files = -1  # set this to a value that works with your RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "dataset_train, means_train, stddevs_train = create_dataset(\n",
    "    \"train\",\n",
    "    shuffle=True,\n",
    "    repeat=-1,\n",
    "    batch_size=batch_size,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_train_files,\n",
    ")\n",
    "dataset_valid, _, _ = create_dataset(\n",
    "    \"valid\",\n",
    "    batch_size=-1,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_valid_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model, regularization_weights = create_model(\n",
    "    (n_constituents, 4),\n",
    "    units=units,\n",
    "    activation=activation,\n",
    "    dropout_rate=dropout_rate,\n",
    "    fs_args=(means_train, stddevs_train),\n",
    ")\n",
    "loss_fns = create_losses(regularization_weights, l2_norm)\n",
    "optimizer, learning_rate = create_optimizer(initial_learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train\n",
    "best_model, metrics = training_loop(\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    model,\n",
    "    loss_fns,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    max_steps=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create the ROC and output plots we defined above to study the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = dataset_train._orig_data[2]\n",
    "labels_valid = dataset_valid._orig_data[2]\n",
    "\n",
    "predictions_train = best_model.predict(dataset_train._orig_data[0])\n",
    "predictions_valid = best_model.predict(dataset_valid._orig_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\n",
    "    (labels_train, labels_valid),\n",
    "    (predictions_train, predictions_valid),\n",
    "    names=(\"train\", \"valid\"),\n",
    ").show()\n",
    "\n",
    "plot_hist(\n",
    "    (predictions_valid[labels_valid[:, 1] == 0][:, 1], predictions_valid[labels_valid[:, 1] == 1][:, 1]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "- We learned about eager execution and graphs.\n",
    "- We know what `tf.function`'s are and how they create and cache graphs by the means of signature tracing.\n",
    "- Feature scaling and the separation of numerical domains between network and physics application were motivated.\n",
    "- We built a data pipeline using TensorFlow datasets.\n",
    "- We created and used a custom training loop using the GradientTape.\n",
    "- We learned that the choice of hyper-parameters is crucial for physics performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-purpose networks: including a jet energy regression (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks can be used for more than one purpose at the same time. Although it is always an option to develop one model for each particular use case, it might be beneficial to rely on a single, multi-purpose network.\n",
    "\n",
    "First, the power of a model to make predictions depends on its ability to extract useful information from input data. This entails the discovery of inner symmetries in usually complex data (physics relations in our case) as well as of (high-dimensional) regions in the phase space that separate between signal and background. Two tasks might require their networks to exploit its input data in a similar fashion, so having a common part seems very natural.\n",
    "\n",
    "Second, training two separate networks for two different purposes provides no handle to control the correlation between their particular outputs. For instance, given the same input example, one network might be able to predict a physics quantity very precisely, while an other network struggles at its specific task. However, given the feedback of the loss function through backpropagation, the training target is actually another source of information that a network eventually exploits. Therefore, the presence of a second training target in the loss function can actually influence the performance to predict the initial target, and this is what we are going to do in the following.\n",
    "\n",
    "Instead of just classifying a jet as either a top or a light jet, we include a regression task, i.e., the prediction of a real physics quantity. For all true top jets, we want to learn the energy of the initial particle that caused the jet. For light jets, we want this energy to be zero.\n",
    "\n",
    "We achieve this by definining a network part that is common to both tasks, as well as two dedicated sets of densely connected layers. The output of these two parts are stacked, resulting in a total of three output nodes: two classification outputs (light-jet-like, top-jet-like) and one regression output (the energy). For the combined loss function, this implies that we need to introduce another hyper-parameter (`reg_norm`) to balance the regression loss with respect to the classification loss.\n",
    "\n",
    "Sounds easy so far, but in fact, there is an issue! As we've seen above, the target energy values span a range from 0 to approximately 2500 GeV. Using the terms introduced above, this numerical *application domain* is not compatible with the *network domain*, so another scaling is required. When doing so, the network is constructed to predict the scaled value which is then manually re-scaled to obtain a value that we *interpret* as an energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new model building function\n",
    "def create_model(input_shape, units=((128, 128), (128, 128), (128, 128)), activation=\"tanh\", dropout_rate=0.,\n",
    "        fs_args=None):\n",
    "    # track weights for later use\n",
    "    weights = []\n",
    "    \n",
    "    # input layer\n",
    "    x = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # feature scaling\n",
    "    if not fs_args:\n",
    "        fs_args = (tf.constant(4 * [0.]), tf.constant(4 * [1.]))\n",
    "    a = FeatureScaling(*fs_args)(x)\n",
    "\n",
    "    # helper to stack layers\n",
    "    def stack_layers(a, units):\n",
    "        for n in units:\n",
    "            # build the layer\n",
    "            layer = tf.keras.layers.Dense(n, use_bias=True, activation=activation)\n",
    "            a = layer(a)\n",
    "\n",
    "            # store the weight matrix for later use\n",
    "            weights.append(layer.kernel)\n",
    "\n",
    "            # add random unit dropout\n",
    "            if dropout_rate:\n",
    "                a = tf.keras.layers.Dropout(dropout_rate)(a)\n",
    "        return a\n",
    "\n",
    "    # layers of the common network part\n",
    "    a_common = stack_layers(a, units[0])\n",
    "    \n",
    "    # layers of the classification part\n",
    "    a_cls = stack_layers(a, units[1])\n",
    "    \n",
    "    # layers of the regression part\n",
    "    a_reg = stack_layers(a, units[2])\n",
    "\n",
    "    # softmax layer for the classification\n",
    "    y_cls = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a_cls)\n",
    "    \n",
    "    # simple dense layer with asinh activation for the regression\n",
    "    # it's not in Keras so use the tf op right away\n",
    "    # https://www.wolframalpha.com/input/?i=draw+arcsinh%28x%29+from+x%3D-10+to+10\n",
    "    y_reg = tf.math.asinh(tf.keras.layers.Dense(1, use_bias=True)(a_reg))\n",
    "    \n",
    "    # stack them to get the total output\n",
    "    y = tf.concat([y_cls, y_reg], axis=-1)\n",
    "    \n",
    "    # build the model\n",
    "    model = tf.keras.Model(inputs=x, outputs=y, name=\"toptagging_and_regression\")\n",
    "\n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new losses\n",
    "def create_losses(energy_mean, energy_stddev, weights, reg_norm=1., l2_norm=0.001):\n",
    "    # cross entropy\n",
    "    @tf.function\n",
    "    def loss_ce_fn(labels, predictions):\n",
    "        # only use the first two values\n",
    "        labels = labels[:, :2]\n",
    "        predictions = predictions[:, :2]\n",
    "        \n",
    "        # ensure proper prediction values before applying log's\n",
    "        predictions = tf.clip_by_value(predictions, 1e-6, 1 - 1e-6)\n",
    "        loss_ce = tf.reduce_mean(-labels * tf.math.log(predictions))\n",
    "        return loss_ce\n",
    "\n",
    "    # MSE loss\n",
    "    @tf.function\n",
    "    def loss_mse_fn(labels, predictions):\n",
    "        # only use the third value\n",
    "        target = labels[:, 2]\n",
    "        prediction = predictions[:, 2]\n",
    "        \n",
    "        # we scale the target so that the network tries to predict\n",
    "        # a value in the exact same domain\n",
    "        target_scaled = (target - energy_mean) / energy_stddev\n",
    "        \n",
    "        # compute the mse loss\n",
    "        loss_mse = tf.reduce_mean((target_scaled - prediction)**2.)\n",
    "\n",
    "        return reg_norm * loss_mse\n",
    "\n",
    "    # l2 loss\n",
    "    @tf.function\n",
    "    def loss_l2_fn(labels, predictions):\n",
    "        # accept labels and predictions although we don't need them\n",
    "        # but this makes it easier to call all loss functions the same way\n",
    "        loss_l2 = sum(tf.reduce_sum(w**2) for w in weights)\n",
    "        \n",
    "        return l2_norm * loss_l2\n",
    "        \n",
    "    # return a dict with all loss function components\n",
    "    return {\"ce\": loss_ce_fn, \"mse\": loss_mse_fn, \"l2\": loss_l2_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "# ACTION REQUIRED\n",
    "n_constituents = 120\n",
    "batch_size = 200\n",
    "reg_norm = 0.\n",
    "l2_norm = 0.001\n",
    "initial_learning_rate = 0.003\n",
    "units = ((128, 128), (128, 128), (128, 128))\n",
    "activation = \"tanh\"\n",
    "dropout_rate = 0.\n",
    "n_train_files = 6\n",
    "n_valid_files = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "dataset_train, means_train, stddevs_train = create_dataset(\n",
    "    \"train\",\n",
    "    shuffle=True,\n",
    "    repeat=-1,\n",
    "    batch_size=batch_size,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_train_files,\n",
    ")\n",
    "dataset_valid, _, _ = create_dataset(\n",
    "    \"valid\",\n",
    "    batch_size=-1,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_valid_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and stddev of the true energy\n",
    "true_e = dataset_train._orig_data[1][:, E]\n",
    "true_e = true_e[true_e > 0]\n",
    "energy_mean = np.mean(true_e)\n",
    "energy_stddev = np.std(true_e)\n",
    "print(f\"energy mean  : {energy_mean:.4f}\")\n",
    "print(f\"energy stddev: {energy_stddev:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model, regularization_weights = create_model(\n",
    "    (n_constituents, 4),\n",
    "    units=units,\n",
    "    activation=activation,\n",
    "    dropout_rate=dropout_rate,\n",
    "    fs_args=(means_train, stddevs_train),\n",
    ")\n",
    "loss_fns = create_losses(energy_mean, energy_stddev, regularization_weights, reg_norm, l2_norm)\n",
    "optimizer, learning_rate = create_optimizer(initial_learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train\n",
    "# (note that *stack_energy* is set to True)\n",
    "best_model, metrics = training_loop(\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    model,\n",
    "    loss_fns,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    max_steps=5000,\n",
    "    stack_energy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model now returns classification outputs and a\n",
    "# *scaled* energy, so create a tf.function that *re-scales* it\n",
    "@tf.function\n",
    "def rescaled_model(c_vectors):\n",
    "    predictions = best_model(c_vectors, training=False)\n",
    "    rescaled_energy = (predictions[:, 2:] * energy_stddev) + energy_mean\n",
    "    return tf.concat([predictions[:, :2], rescaled_energy], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels and predictions\n",
    "labels_valid = dataset_valid._orig_data[2]\n",
    "true_e_valid = dataset_valid._orig_data[1][:, E]\n",
    "predictions_valid = rescaled_model(dataset_valid._orig_data[0]).numpy()\n",
    "\n",
    "# get mask to select true tops\n",
    "is_top = labels_valid[:, 1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions\n",
    "plot_hist(\n",
    "    (predictions_valid[:, 1][~is_top], predictions_valid[:, 1][is_top]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ").show()\n",
    "\n",
    "plot_hist(\n",
    "    (true_e_valid[is_top], predictions_valid[:, 2][is_top]),\n",
    "    names=(\"Truth\", \"Prediction\"),\n",
    "    xlabel=\"Top energy / GeV\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize both output targets, start with the set of hyper-parameters that you found in the previous task. Then, try to change the `reg_norm` value to see its effect on either the classification or regression output.\n",
    "\n",
    "Feel free to also redefine loss functions, add further concepts (as above) and exchange your ideas with other participants in the [discord channel](https://discord.com/channels/720916736983171133/745610366012227606)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Authors\n",
    "\n",
    "- [Marcel Rieger](mailto:marcel.rieger@cern.ch)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "rise": {
   "controls": false,
   "footer": "&nbsp;&nbsp; <b>CMSDAS 2020</b> | Machine Learning Exercise | Top Tagging",
   "scroll": true,
   "slideNumber": "c",
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
